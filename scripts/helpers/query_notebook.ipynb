{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sn14-query-demonstration-helper\n",
    "This notebook is meant to be used as an example to demonstrate how the subnet miners can be queried and how the validation process works. The implementation is slightly different from the normal implementation in the validators, since the logic is not 1:1 transferrable into a Jupyter Notebook. The changes are outlined in the code.\n",
    "\n",
    "The first parts of the demonstrations are more about generic usage, while the second part answers to the questions within the subnet demo notebook (https://github.com/opentensor/subnet_demo_stack/blob/main/demo_template.ipynb).\n",
    "\n",
    "Any issues with the demo notebook, please reach out directly to @ceterum_ or @m4k1_kun via Discord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules to demonstrate the validator\n",
    "import bittensor as bt\n",
    "from llm_defender.base.protocol importLLMDefender.SubnetProtocol\n",
    "from llm_defender.core.validators.validator import SubnetValidator\n",
    "from argparse import ArgumentParser\n",
    "from llm_defender.base.utils import sign_data\n",
    "from torch import argmax, argmin, nonzero, masked_select\n",
    "from uuid import uuid4\n",
    "from json import dumps\n",
    "from secrets import token_hex\n",
    "from time import time\n",
    "from sys import exit\n",
    "import requests\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup subnet-related objects required to query the network\n",
    "\n",
    "# Subnet parameters\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--alpha\",\n",
    "    default=0.9,\n",
    "    type=float,\n",
    "    help=\"The weight moving average scoring.\",\n",
    ")\n",
    "parser.add_argument(\"--netuid\", type=int, default=14, help=\"The chain subnet uid.\")\n",
    "\n",
    "# Define the bittensor configuration parameters here\n",
    "parser.add_argument(\"--wallet.name\", type=str, default=\"default\")\n",
    "parser.add_argument(\"--wallet.hotkey\", type=str, default=\"default\")\n",
    "parser.add_argument(\"--subtensor.network\", type=str, default=\"finney\")\n",
    "\n",
    "\n",
    "# Setup Subnet validator\n",
    "subnet_validator = SubnetValidator(parser=parser)\n",
    "\n",
    "# Apply configuration\n",
    "subnet_validator.apply_config(bt_classes=[bt.subtensor, bt.logging, bt.wallet])\n",
    "\n",
    "# Initialize validator\n",
    "wallet, subtensor, dendrite, metagraph = subnet_validator.setup_bittensor_objects(\n",
    "    subnet_validator.neuron_config\n",
    ")\n",
    "subnet_validator.wallet = wallet\n",
    "subnet_validator.subtensor = subtensor\n",
    "subnet_validator.dendrite = dendrite\n",
    "subnet_validator.metagraph = metagraph\n",
    "subnet_validator.init_default_scores()\n",
    "subnet_validator.remote_logging = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions are needed resolve certain parameters before sending\n",
    "# out the query. These are not part of the standard subnet\n",
    "# implementation.\n",
    "\n",
    "def get_dataset_entries(seed=42, samples=50):\n",
    "    dataset = load_dataset(\"synapsecai/synthetic-prompt-injections\")\n",
    "    test_dataset = dataset[\"test\"]\n",
    "\n",
    "    dataset = test_dataset.shuffle(seed=seed)\n",
    "    samples = test_dataset.select(range(samples))\n",
    "\n",
    "    return samples\n",
    "\n",
    "def get_top_miner() -> bt.NeuronInfo:\n",
    "    \"\"\"This function returns the NeuronInfo for the top performing miner\n",
    "    based on incentive\"\"\"\n",
    "\n",
    "    # Get incentives and the top performing UID\n",
    "    incentives = subnet_validator.metagraph.I\n",
    "    uid = argmax(incentives)\n",
    "    print(f\"Top UID: {uid} with incentive: {incentives[uid]}\")\n",
    "\n",
    "    # Return AxonInfo based on the UID\n",
    "    neuron_info = subnet_validator.metagraph.neurons[uid]\n",
    "    return neuron_info\n",
    "\n",
    "def get_bottom_miner() -> bt.NeuronInfo:\n",
    "    \"\"\"This function returns the NeuronInfo for the top performing miner\n",
    "    based on incentive\"\"\"\n",
    "\n",
    "    # Get incentives and the top performing UID\n",
    "    incentives = subnet_validator.metagraph.I\n",
    "    non_zero_mask = incentives > 0.003 # limit to miners that are performing somewhat ok, there are couple of mining validators that do not response properly\n",
    "    min_non_zero_value = masked_select(incentives, non_zero_mask).min()\n",
    "    \n",
    "    # Step 3: Get the index of the minimum non-zero value\n",
    "    uid = (incentives == min_non_zero_value).nonzero(as_tuple=True)[0][0]\n",
    "    \n",
    "    print(f\"Bottom UID: {uid} with incentive: {incentives[uid]}\")\n",
    "\n",
    "    # Return AxonInfo based on the UID\n",
    "    neuron_info = subnet_validator.metagraph.neurons[uid]\n",
    "    return neuron_info\n",
    "\n",
    "\n",
    "def standard_response_processing(responses, queries, uuids, axon_to_query, no_output=False) -> list:\n",
    "    \"\"\"This functions performs the standard response processing and\n",
    "    returns list of processed responses\"\"\"\n",
    "\n",
    "    processed_responses = []\n",
    "    for i, entry in enumerate(responses):\n",
    "        if not no_output:\n",
    "            print(\n",
    "                f\"Received non-empty response from the miner:\\n{dumps(entry.output, indent=4)}\"\n",
    "            )\n",
    "\n",
    "        # The process_response function is responsible for handling valid\n",
    "        # responses. It is executed if any one of the responses is non-empty.\n",
    "        response_data = subnet_validator.process_responses(\n",
    "            query=queries[i],\n",
    "            processed_uids=[axon_to_query.uid],\n",
    "            responses=[entry],\n",
    "            synapse_uuid=uuids[i],\n",
    "        )\n",
    "\n",
    "        processed_responses.append(response_data)\n",
    "\n",
    "        for res in response_data:\n",
    "            if subnet_validator.miner_responses:\n",
    "                if res[\"hotkey\"] in subnet_validator.miner_responses:\n",
    "                    subnet_validator.miner_responses[res[\"hotkey\"]].append(res)\n",
    "                else:\n",
    "                    subnet_validator.miner_responses[res[\"hotkey\"]] = [res]\n",
    "            else:\n",
    "                subnet_validator.miner_responses = {}\n",
    "                subnet_validator.miner_responses[res[\"hotkey\"]] = [res]\n",
    "    \n",
    "    return processed_responses\n",
    "\n",
    "def calculate_response_metrics(hotkey) -> dict:\n",
    "    \"\"\"This function calculates some standard metrics for the miner\n",
    "    responses contained in the local memory\"\"\"\n",
    "\n",
    "    data = subnet_validator.miner_responses[hotkey]\n",
    "    \n",
    "    labels = []\n",
    "    classifications = []\n",
    "\n",
    "    for i,entry in enumerate(data):\n",
    "        if entry[\"target\"] is None or \"confidence\" not in entry[\"response\"].keys() or entry[\"response\"][\"confidence\"] is None:\n",
    "            continue\n",
    "        labels.append(round(entry[\"target\"]))\n",
    "        classifications.append(round(entry[\"response\"][\"confidence\"]))\n",
    "\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, classifications)\n",
    "    precision = precision_score(labels, classifications)\n",
    "    recall = recall_score(labels, classifications)\n",
    "    f1 = f1_score(labels, classifications)\n",
    "\n",
    "    res = {\n",
    "        \"hotkey\": hotkey,\n",
    "        \"count\": len(data),\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        },\n",
    "        \"scoring_averages\": {\n",
    "            \"total_score\": sum(entry[\"scored_response\"][\"scores\"][\"total\"] for entry in data) / len(data) if data else None,\n",
    "            \"distance_score\": sum(entry[\"scored_response\"][\"scores\"][\"distance\"] for entry in data) / len(data) if data else None,\n",
    "            \"speed_score\": sum(entry[\"scored_response\"][\"scores\"][\"speed\"] for entry in data) / len(data) if data else None,\n",
    "            \"raw_distance\": sum(entry[\"scored_response\"][\"raw_scores\"][\"distance\"] for entry in data) / len(data) if data else None,\n",
    "            \"raw_speed\": sum(entry[\"scored_response\"][\"raw_scores\"][\"speed\"] for entry in data) / len(data) if data else None,\n",
    "            \"distance_penalty\": sum(entry[\"scored_response\"][\"penalties\"][\"distance\"] for entry in data) / len(data) if data else None,\n",
    "            \"speed_penalty\": sum(entry[\"scored_response\"][\"penalties\"][\"speed\"] for entry in data) / len(data) if data else None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup variables that are needed throughout the execution\n",
    "axon_to_query = get_top_miner()\n",
    "print(f\"Axon to query: {axon_to_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block executes a normal query where the prompt is fetched\n",
    "# through the Prompt API. This is the primary query executed by the\n",
    "# validators.\n",
    "\n",
    "# Get the query to send for the miner. The query contains the metadata\n",
    "# associated with the prompt that is to be analyzed by the miners. In\n",
    "# order to query the prompt API, the hotkey must be registered in the\n",
    "# subnet 14 and have at least 20k staked TAO.\n",
    "\n",
    "# Send queries to the miner\n",
    "#\n",
    "# DANGER: There is a rate-limit of 100 requests every 10 minutes. If you\n",
    "# are executing this notebook from a host with the same IP with your\n",
    "# actual validator, you are at risk of being rate-limited.\n",
    "#\n",
    "# Please use the dataset-based approach below for queries with multiple\n",
    "# prompts. Using two prompts should be enough to be able to demonstrate\n",
    "# how the process works. The dataset-based approach below has more detailed examples\n",
    "\n",
    "async def normal_query(normal_axon, n=2):\n",
    "    responses = []\n",
    "    queries = []\n",
    "    uuids = []\n",
    "    for _ in range(0, n):\n",
    "        # UUID is used to uniquely identify the request\n",
    "        synapse_uuid = str(uuid4())\n",
    "        uuids.append(synapse_uuid)\n",
    "\n",
    "        # Miner hotkeys are used to by the Prompt API to create prompts for the\n",
    "        # miners to fetch. Each prompt can be fetched only once.\n",
    "        miner_hotkeys = [normal_axon.hotkey]\n",
    "\n",
    "        # Executing the serve_prompt method fetches the query metadata from the Prompt API\n",
    "        query = subnet_validator.serve_prompt(\n",
    "            synapse_uuid=synapse_uuid, miner_hotkeys=miner_hotkeys\n",
    "        )\n",
    "        queries.append(query)\n",
    "\n",
    "        print(f\"Query metadata:\\n{dumps(query, indent=4)}\")\n",
    "\n",
    "        # Prepare the query. The query is signed and includes a nonce, to prevent replay of validator requests\n",
    "        nonce = token_hex(24)\n",
    "        timestamp = str(int(time()))\n",
    "        signature = LLMDefender.sign_data(\n",
    "            hotkey=subnet_validator.wallet.hotkey,\n",
    "            data=f\"{synapse_uuid}{nonce}{subnet_validator.wallet.hotkey.ss58_address}{timestamp}\",\n",
    "        )\n",
    "        miner_res = await subnet_validator.dendrite(\n",
    "            normal_axon.axon_info,\n",
    "           LLMDefender.SubnetProtocol(\n",
    "                analyzer=query[\"analyzer\"],\n",
    "                subnet_version=subnet_validator.subnet_version,\n",
    "                synapse_uuid=synapse_uuid,\n",
    "                synapse_signature=signature,\n",
    "                synapse_nonce=nonce,\n",
    "                synapse_timestamp=timestamp,\n",
    "            ),\n",
    "            timeout=6,\n",
    "            deserialize=True,\n",
    "        )\n",
    "        responses.append(miner_res)\n",
    "    \n",
    "    return responses, queries, uuids\n",
    "\n",
    "miner_responses, queries, uuids = await normal_query(normal_axon=axon_to_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_miner_responses(miner_responses, queries, uuids, axon_to_process, no_output=False):\n",
    "    # This code block executes the built-in process-method for the responses\n",
    "    # retrieved by the previous code block.\n",
    "\n",
    "    # If all of the responses in the list of returned responses are empty\n",
    "    # the validator would normally set the score for the queries to 0.0.\n",
    "    if not miner_responses or all(item.output is None for item in miner_responses):\n",
    "        print(f\"Received empty responses: {miner_responses}\")\n",
    "        exit(-1)\n",
    "\n",
    "    processed_responses = standard_response_processing(miner_responses, queries, uuids, axon_to_process, no_output)\n",
    "\n",
    "    # After processing the responses, the validator would normally update\n",
    "    # its local knowledge of the miner responses. The local knowledge of the\n",
    "    # miner responses are used within the reward mechanism.\n",
    "\n",
    "    if not no_output:\n",
    "        print(f\"Processed response:\\n{dumps(processed_responses, indent=4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block starts the execution of the non-standard way of\n",
    "# querying the miners. This code block creates prompts for the miners to\n",
    "# analyze rather than using the synthetic prompts generated by the\n",
    "# prompt api. This could be considered as an early prototype of the\n",
    "# subnet-api the subnet owners are building.\n",
    "\n",
    "\n",
    "def get_subnet_api(synapse_uuid, data) -> dict:\n",
    "    \"\"\"Retrieves a prompt from the prompt API\"\"\"\n",
    "\n",
    "    nonce = token_hex(24)\n",
    "    timestamp = str(int(time()))\n",
    "    signature = LLMDefender.sign_data(\n",
    "        hotkey=subnet_validator.wallet.hotkey,\n",
    "        data=f\"{synapse_uuid}{nonce}{timestamp}\",\n",
    "    )\n",
    "\n",
    "    headers = {\n",
    "        \"X-Hotkey\": subnet_validator.wallet.hotkey.ss58_address,\n",
    "        \"X-Signature\": signature,\n",
    "        \"X-SynapseUUID\": synapse_uuid,\n",
    "        \"X-Timestamp\": timestamp,\n",
    "        \"X-Nonce\": nonce,\n",
    "        \"X-Version\": str(40),\n",
    "        # \"X-Version\": str(subnet_validator.subnet_version),\n",
    "        \"X-API-Key\": subnet_validator.wallet.hotkey.ss58_address,\n",
    "    }\n",
    "\n",
    "    prompt_api_url = \"https://subnet-api.synapsec.ai/subnet\"\n",
    "    try:\n",
    "        # get prompt\n",
    "        res = requests.post(\n",
    "            url=prompt_api_url, headers=headers, data=dumps(data), timeout=12\n",
    "        )\n",
    "        # check for correct status code\n",
    "        if res.status_code == 201:\n",
    "            # get prompt entry from the API output\n",
    "            prompt_entry = res.json()\n",
    "            # check to make sure prompt is valid\n",
    "            bt.logging.trace(f\"Loaded remote prompt to serve to miners: {prompt_entry}\")\n",
    "            return prompt_entry\n",
    "\n",
    "        else:\n",
    "            bt.logging.warning(\n",
    "                f\"Unable to get prompt from the Prompt API: HTTP/{res.status_code} - {res.json()}\"\n",
    "            )\n",
    "    except requests.exceptions.ReadTimeout as e:\n",
    "        print(e)\n",
    "        bt.logging.error(f\"Prompt API request timed out: {e}\")\n",
    "    except requests.exceptions.JSONDecodeError as e:\n",
    "        print(e)\n",
    "        bt.logging.error(f\"Unable to read the response from the prompt API: {e}\")\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        print(e)\n",
    "        bt.logging.error(f\"Unable to connect to the prompt API: {e}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        bt.logging.error(f\"Generic error during request: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup prompts and labels to be queried. The dataset is used as a\n",
    "# reference contains prompts that have already been served to the\n",
    "# network, so it is likely a top miner is pretty good at classifying the\n",
    "# prompts.\n",
    "\n",
    "async def subnet_api_demo_query(demo_axon, no_output=False):\n",
    "    samples = get_dataset_entries()\n",
    "    \n",
    "    # Now, separate the text and prompts into two lists\n",
    "    prompts = [sample['text'] for sample in samples]\n",
    "    labels = [1 if sample['label'] == 'malicious' else 0 for sample in samples]\n",
    "\n",
    "    # Store miner responses and queries in a list\n",
    "    responses = []\n",
    "    queries = []\n",
    "    uuids = []\n",
    "\n",
    "    # Send queries to miners\n",
    "    for i,_ in enumerate(prompts):\n",
    "        \n",
    "        # UUID is used to uniquely identify the request\n",
    "        synapse_uuid = str(uuid4())\n",
    "        uuids.append(synapse_uuid)\n",
    "        \n",
    "        # Prepare data object to push to Prompt API\n",
    "        data = {\n",
    "            \"hotkey\": demo_axon.hotkey,\n",
    "            \"prompt\": prompts[i],\n",
    "            \"label\": str(labels[i]),\n",
    "            \"analyzer\": \"Prompt Injection\",\n",
    "        }\n",
    "        \n",
    "        # Push prompt to prompt API and get the query metadata\n",
    "        query = get_subnet_api(synapse_uuid=synapse_uuid, data=data)\n",
    "        queries.append(query)\n",
    "\n",
    "        if not no_output:\n",
    "            print(f\"Query metadata:\\n{dumps(query, indent=4)}\")\n",
    "\n",
    "        # Prepare the query. The query is signed and includes a nonce, to prevent replay of validator requests\n",
    "        nonce = token_hex(24)\n",
    "        timestamp = str(int(time()))\n",
    "        signature = LLMDefender.sign_data(\n",
    "            hotkey=subnet_validator.wallet.hotkey,\n",
    "            data=f\"{synapse_uuid}{nonce}{subnet_validator.wallet.hotkey.ss58_address}{timestamp}\",\n",
    "        )\n",
    "\n",
    "        miner_res = await subnet_validator.dendrite(\n",
    "            demo_axon.axon_info,\n",
    "           LLMDefender.SubnetProtocol(\n",
    "                analyzer=query[\"analyzer\"],\n",
    "                subnet_version=subnet_validator.subnet_version,\n",
    "                synapse_uuid=synapse_uuid,\n",
    "                synapse_signature=signature,\n",
    "                synapse_nonce=nonce,\n",
    "                synapse_timestamp=timestamp,\n",
    "            ),\n",
    "            timeout=6,\n",
    "            deserialize=True,\n",
    "        )\n",
    "        responses.append(miner_res)\n",
    "\n",
    "    return responses, queries, uuids\n",
    "    \n",
    "miner_responses, queries, uuids = await subnet_api_demo_query(demo_axon=axon_to_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_miner_responses(miner_responses, queries, uuids, axon_to_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print miner metrics from the stored miner responses\n",
    "metrics = calculate_response_metrics(hotkey=axon_to_query.hotkey)\n",
    "print(dumps(metrics,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A) Demonstrate the quality of the communication between miners and validators coming from the top miner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The communication between miner and validator can be demonstrated by executing the normal_query() function. This is a representative of the normal behavior within the subnet.\n",
    "# There are four prints in this cell block. The \"Query Metadata\" is the metadata the validator uses to score the miner response. \n",
    "# The second print contains the actual response from the miner. \n",
    "# The third one displays the scored response (raw_distance and raw_speed are the key fields to look at: raw_distance equals to absolute distance to correct label and the raw_speed is calculated based on the response speed (response_speed/max_timeout))\n",
    "\n",
    "# Get the top miner\n",
    "top_miner = get_top_miner()\n",
    "\n",
    "# Query the miner\n",
    "miner_responses, queries, uuids = await normal_query(normal_axon=top_miner, n=1)\n",
    "\n",
    "# Process the response from the miner\n",
    "process_miner_responses(miner_responses, queries, uuids, top_miner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(B) Justify the difference in incentive for miners in different tiers (eg.quantile 1 VS quantile 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our scoring is based purely on the score calculated from the raw_distance and raw_speed. There are no performance buckets, but the difference in the scoring is determined by the response quality, effectively the average accuracy.\n",
    "# This block uses the subnet_api_demo_query() function to get top and bottom miner to process 50 prompts and displays \n",
    "# Bulk-querying is not yet supported by the miners, so sending out the prompts and processing them can take 60-90 seconds.\n",
    "\n",
    "# Before we proceed, we must clear our local knowledge of the miner replies to not cause deviation in the score calculations\n",
    "subnet_validator.miner_responses = None\n",
    "\n",
    "# Get the top miner\n",
    "top_miner = get_top_miner()\n",
    "bottom_miner = get_bottom_miner()\n",
    "\n",
    "# Query the top miner\n",
    "top_miner_responses, top_queries, top_uuids = await subnet_api_demo_query(demo_axon=top_miner, no_output=True)\n",
    "\n",
    "# Process the response from the top miner\n",
    "process_miner_responses(top_miner_responses, top_queries, top_uuids, top_miner, True)\n",
    "\n",
    "# Query the bottom miner\n",
    "bottom_miner_responses, bottom_queries, bottom_uuids = await subnet_api_demo_query(demo_axon=bottom_miner, no_output=True)\n",
    "\n",
    "# Process the response from the bottom miner\n",
    "process_miner_responses(bottom_miner_responses, bottom_queries, bottom_uuids, bottom_miner, True)\n",
    "\n",
    "# Print metrics\n",
    "# --Note that when using the dataset in huggingface, there is a high possibility that all of the metrics equal to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print miner metrics from the stored miner responses for top miner\n",
    "metrics = calculate_response_metrics(hotkey=top_miner.hotkey)\n",
    "print(dumps(metrics,indent=4))\n",
    "\n",
    "# Print miner metrics from the stored miner responses for bottom miner\n",
    "metrics = calculate_response_metrics(hotkey=bottom_miner.hotkey)\n",
    "print(dumps(metrics,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C) (If applicable) Show the landscape and variety of miners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not relevant for our subnet. This is somewhat demonstrated by the cell for (B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(D) (If applicable) Demonstrate the effectiveness of the scoring mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is also covered by the cell for (B) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(E) (If applicable) Show the dataset that was used by the validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/joonas/bt-dev/llm-defender-subnet/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# The validator uses a mix of dataset-based prompts and synthetic prompts that are generated out-of-bounds and pushed to an SQS queue used by the prompt API.\n",
    "# The prompt API issues prompts to validators by using 50% dataset and 50% unique synthetic prompts (which have not been seen before sending out to the miners).\n",
    "# Correctly classifying a prompt from dataset is given weight of 10% (0.1 multiplier on score change) and unique prompts are given weight of 100% (1.0 multiplier in scoring)\n",
    "# Datasets that are rotated ouf of the prompt API are published in Huggingface to allow miners to easier fine-tune their models based on formerly analyzed data. \n",
    "\n",
    "samples = get_dataset_entries()\n",
    "print(dumps(samples[:10],indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(F) (If applicable) Show the use of any API and/or links to a frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The APIs used by the subnet are demonstrated within the previous cells of this notebook. The subnet has three APIs:\n",
    "# - Prompt API \n",
    "#   - Used to push synthetically generated and dataset-based prompts to validators (GPT4)\n",
    "#   - Temporary solution pending for the development of robust solution to generate synthetic data on the validators, most likely by utilizing other subnets to generate the data. \n",
    "#   - There are both ethical concerns and technical issues we have not yet managed to address.\n",
    "# - Fetch API \n",
    "#   - Used by the miners to fetch the prompts \n",
    "#   - Temporary solution to prevent relay attacks\n",
    "# - Subnet API\n",
    "#   - Used by validators to push prompts into the fetch API\n",
    "#   - To be published as open source Subnet API any validator can run (once fetch API has been replaced with a solution that can be run on the validator)\n",
    "# These are documented at: https://docs.synapsec.ai\n",
    "# Code can be shared upon request, but we are not yet ready to share these to everyone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
